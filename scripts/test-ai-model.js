import { pipeline } from "@xenova/transformers"

async function testModel() {
  console.log("ğŸ¤– Probando modelo de IA en espaÃ±ol...")

  try {
    console.log("ğŸ“¥ Cargando modelo PlanTL-GOB-ES/gpt2-base-bne...")

    const generator = await pipeline("text-generation", "PlanTL-GOB-ES/gpt2-base-bne")

    console.log("âœ… Modelo cargado correctamente!")

    const testPrompts = ["Hola, Â¿cÃ³mo estÃ¡s?", "Â¿QuÃ© es la inteligencia artificial?", "CuÃ©ntame sobre EspaÃ±a"]

    for (const prompt of testPrompts) {
      console.log(`\nğŸ”¤ Probando: "${prompt}"`)

      const result = await generator(`Usuario: ${prompt}\nAsistente:`, {
        max_new_tokens: 50,
        temperature: 0.7,
        do_sample: true,
        top_p: 0.9,
      })

      const response = result[0].generated_text.replace(`Usuario: ${prompt}\nAsistente:`, "").trim()
      console.log(`ğŸ’¬ Respuesta: ${response}`)
    }

    console.log("\nğŸ‰ Â¡Modelo funcionando correctamente!")
  } catch (error) {
    console.error("âŒ Error probando el modelo:", error)
  }
}

testModel()
